{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[[[i+j] for i in range(5)] for j in range(100)]\n",
    "data1=[[[i+j] for i in range(5)] for j in range(100)]\n",
    "target=[(i+5)/5 for i in range(100)]\n",
    "data=np.array(data)\n",
    "target=np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0],\n",
       "        [  1],\n",
       "        [  2],\n",
       "        [  3],\n",
       "        [  4]],\n",
       "\n",
       "       [[  1],\n",
       "        [  2],\n",
       "        [  3],\n",
       "        [  4],\n",
       "        [  5]],\n",
       "\n",
       "       [[  2],\n",
       "        [  3],\n",
       "        [  4],\n",
       "        [  5],\n",
       "        [  6]],\n",
       "\n",
       "       [[  3],\n",
       "        [  4],\n",
       "        [  5],\n",
       "        [  6],\n",
       "        [  7]],\n",
       "\n",
       "       [[  4],\n",
       "        [  5],\n",
       "        [  6],\n",
       "        [  7],\n",
       "        [  8]],\n",
       "\n",
       "       [[  5],\n",
       "        [  6],\n",
       "        [  7],\n",
       "        [  8],\n",
       "        [  9]],\n",
       "\n",
       "       [[  6],\n",
       "        [  7],\n",
       "        [  8],\n",
       "        [  9],\n",
       "        [ 10]],\n",
       "\n",
       "       [[  7],\n",
       "        [  8],\n",
       "        [  9],\n",
       "        [ 10],\n",
       "        [ 11]],\n",
       "\n",
       "       [[  8],\n",
       "        [  9],\n",
       "        [ 10],\n",
       "        [ 11],\n",
       "        [ 12]],\n",
       "\n",
       "       [[  9],\n",
       "        [ 10],\n",
       "        [ 11],\n",
       "        [ 12],\n",
       "        [ 13]],\n",
       "\n",
       "       [[ 10],\n",
       "        [ 11],\n",
       "        [ 12],\n",
       "        [ 13],\n",
       "        [ 14]],\n",
       "\n",
       "       [[ 11],\n",
       "        [ 12],\n",
       "        [ 13],\n",
       "        [ 14],\n",
       "        [ 15]],\n",
       "\n",
       "       [[ 12],\n",
       "        [ 13],\n",
       "        [ 14],\n",
       "        [ 15],\n",
       "        [ 16]],\n",
       "\n",
       "       [[ 13],\n",
       "        [ 14],\n",
       "        [ 15],\n",
       "        [ 16],\n",
       "        [ 17]],\n",
       "\n",
       "       [[ 14],\n",
       "        [ 15],\n",
       "        [ 16],\n",
       "        [ 17],\n",
       "        [ 18]],\n",
       "\n",
       "       [[ 15],\n",
       "        [ 16],\n",
       "        [ 17],\n",
       "        [ 18],\n",
       "        [ 19]],\n",
       "\n",
       "       [[ 16],\n",
       "        [ 17],\n",
       "        [ 18],\n",
       "        [ 19],\n",
       "        [ 20]],\n",
       "\n",
       "       [[ 17],\n",
       "        [ 18],\n",
       "        [ 19],\n",
       "        [ 20],\n",
       "        [ 21]],\n",
       "\n",
       "       [[ 18],\n",
       "        [ 19],\n",
       "        [ 20],\n",
       "        [ 21],\n",
       "        [ 22]],\n",
       "\n",
       "       [[ 19],\n",
       "        [ 20],\n",
       "        [ 21],\n",
       "        [ 22],\n",
       "        [ 23]],\n",
       "\n",
       "       [[ 20],\n",
       "        [ 21],\n",
       "        [ 22],\n",
       "        [ 23],\n",
       "        [ 24]],\n",
       "\n",
       "       [[ 21],\n",
       "        [ 22],\n",
       "        [ 23],\n",
       "        [ 24],\n",
       "        [ 25]],\n",
       "\n",
       "       [[ 22],\n",
       "        [ 23],\n",
       "        [ 24],\n",
       "        [ 25],\n",
       "        [ 26]],\n",
       "\n",
       "       [[ 23],\n",
       "        [ 24],\n",
       "        [ 25],\n",
       "        [ 26],\n",
       "        [ 27]],\n",
       "\n",
       "       [[ 24],\n",
       "        [ 25],\n",
       "        [ 26],\n",
       "        [ 27],\n",
       "        [ 28]],\n",
       "\n",
       "       [[ 25],\n",
       "        [ 26],\n",
       "        [ 27],\n",
       "        [ 28],\n",
       "        [ 29]],\n",
       "\n",
       "       [[ 26],\n",
       "        [ 27],\n",
       "        [ 28],\n",
       "        [ 29],\n",
       "        [ 30]],\n",
       "\n",
       "       [[ 27],\n",
       "        [ 28],\n",
       "        [ 29],\n",
       "        [ 30],\n",
       "        [ 31]],\n",
       "\n",
       "       [[ 28],\n",
       "        [ 29],\n",
       "        [ 30],\n",
       "        [ 31],\n",
       "        [ 32]],\n",
       "\n",
       "       [[ 29],\n",
       "        [ 30],\n",
       "        [ 31],\n",
       "        [ 32],\n",
       "        [ 33]],\n",
       "\n",
       "       [[ 30],\n",
       "        [ 31],\n",
       "        [ 32],\n",
       "        [ 33],\n",
       "        [ 34]],\n",
       "\n",
       "       [[ 31],\n",
       "        [ 32],\n",
       "        [ 33],\n",
       "        [ 34],\n",
       "        [ 35]],\n",
       "\n",
       "       [[ 32],\n",
       "        [ 33],\n",
       "        [ 34],\n",
       "        [ 35],\n",
       "        [ 36]],\n",
       "\n",
       "       [[ 33],\n",
       "        [ 34],\n",
       "        [ 35],\n",
       "        [ 36],\n",
       "        [ 37]],\n",
       "\n",
       "       [[ 34],\n",
       "        [ 35],\n",
       "        [ 36],\n",
       "        [ 37],\n",
       "        [ 38]],\n",
       "\n",
       "       [[ 35],\n",
       "        [ 36],\n",
       "        [ 37],\n",
       "        [ 38],\n",
       "        [ 39]],\n",
       "\n",
       "       [[ 36],\n",
       "        [ 37],\n",
       "        [ 38],\n",
       "        [ 39],\n",
       "        [ 40]],\n",
       "\n",
       "       [[ 37],\n",
       "        [ 38],\n",
       "        [ 39],\n",
       "        [ 40],\n",
       "        [ 41]],\n",
       "\n",
       "       [[ 38],\n",
       "        [ 39],\n",
       "        [ 40],\n",
       "        [ 41],\n",
       "        [ 42]],\n",
       "\n",
       "       [[ 39],\n",
       "        [ 40],\n",
       "        [ 41],\n",
       "        [ 42],\n",
       "        [ 43]],\n",
       "\n",
       "       [[ 40],\n",
       "        [ 41],\n",
       "        [ 42],\n",
       "        [ 43],\n",
       "        [ 44]],\n",
       "\n",
       "       [[ 41],\n",
       "        [ 42],\n",
       "        [ 43],\n",
       "        [ 44],\n",
       "        [ 45]],\n",
       "\n",
       "       [[ 42],\n",
       "        [ 43],\n",
       "        [ 44],\n",
       "        [ 45],\n",
       "        [ 46]],\n",
       "\n",
       "       [[ 43],\n",
       "        [ 44],\n",
       "        [ 45],\n",
       "        [ 46],\n",
       "        [ 47]],\n",
       "\n",
       "       [[ 44],\n",
       "        [ 45],\n",
       "        [ 46],\n",
       "        [ 47],\n",
       "        [ 48]],\n",
       "\n",
       "       [[ 45],\n",
       "        [ 46],\n",
       "        [ 47],\n",
       "        [ 48],\n",
       "        [ 49]],\n",
       "\n",
       "       [[ 46],\n",
       "        [ 47],\n",
       "        [ 48],\n",
       "        [ 49],\n",
       "        [ 50]],\n",
       "\n",
       "       [[ 47],\n",
       "        [ 48],\n",
       "        [ 49],\n",
       "        [ 50],\n",
       "        [ 51]],\n",
       "\n",
       "       [[ 48],\n",
       "        [ 49],\n",
       "        [ 50],\n",
       "        [ 51],\n",
       "        [ 52]],\n",
       "\n",
       "       [[ 49],\n",
       "        [ 50],\n",
       "        [ 51],\n",
       "        [ 52],\n",
       "        [ 53]],\n",
       "\n",
       "       [[ 50],\n",
       "        [ 51],\n",
       "        [ 52],\n",
       "        [ 53],\n",
       "        [ 54]],\n",
       "\n",
       "       [[ 51],\n",
       "        [ 52],\n",
       "        [ 53],\n",
       "        [ 54],\n",
       "        [ 55]],\n",
       "\n",
       "       [[ 52],\n",
       "        [ 53],\n",
       "        [ 54],\n",
       "        [ 55],\n",
       "        [ 56]],\n",
       "\n",
       "       [[ 53],\n",
       "        [ 54],\n",
       "        [ 55],\n",
       "        [ 56],\n",
       "        [ 57]],\n",
       "\n",
       "       [[ 54],\n",
       "        [ 55],\n",
       "        [ 56],\n",
       "        [ 57],\n",
       "        [ 58]],\n",
       "\n",
       "       [[ 55],\n",
       "        [ 56],\n",
       "        [ 57],\n",
       "        [ 58],\n",
       "        [ 59]],\n",
       "\n",
       "       [[ 56],\n",
       "        [ 57],\n",
       "        [ 58],\n",
       "        [ 59],\n",
       "        [ 60]],\n",
       "\n",
       "       [[ 57],\n",
       "        [ 58],\n",
       "        [ 59],\n",
       "        [ 60],\n",
       "        [ 61]],\n",
       "\n",
       "       [[ 58],\n",
       "        [ 59],\n",
       "        [ 60],\n",
       "        [ 61],\n",
       "        [ 62]],\n",
       "\n",
       "       [[ 59],\n",
       "        [ 60],\n",
       "        [ 61],\n",
       "        [ 62],\n",
       "        [ 63]],\n",
       "\n",
       "       [[ 60],\n",
       "        [ 61],\n",
       "        [ 62],\n",
       "        [ 63],\n",
       "        [ 64]],\n",
       "\n",
       "       [[ 61],\n",
       "        [ 62],\n",
       "        [ 63],\n",
       "        [ 64],\n",
       "        [ 65]],\n",
       "\n",
       "       [[ 62],\n",
       "        [ 63],\n",
       "        [ 64],\n",
       "        [ 65],\n",
       "        [ 66]],\n",
       "\n",
       "       [[ 63],\n",
       "        [ 64],\n",
       "        [ 65],\n",
       "        [ 66],\n",
       "        [ 67]],\n",
       "\n",
       "       [[ 64],\n",
       "        [ 65],\n",
       "        [ 66],\n",
       "        [ 67],\n",
       "        [ 68]],\n",
       "\n",
       "       [[ 65],\n",
       "        [ 66],\n",
       "        [ 67],\n",
       "        [ 68],\n",
       "        [ 69]],\n",
       "\n",
       "       [[ 66],\n",
       "        [ 67],\n",
       "        [ 68],\n",
       "        [ 69],\n",
       "        [ 70]],\n",
       "\n",
       "       [[ 67],\n",
       "        [ 68],\n",
       "        [ 69],\n",
       "        [ 70],\n",
       "        [ 71]],\n",
       "\n",
       "       [[ 68],\n",
       "        [ 69],\n",
       "        [ 70],\n",
       "        [ 71],\n",
       "        [ 72]],\n",
       "\n",
       "       [[ 69],\n",
       "        [ 70],\n",
       "        [ 71],\n",
       "        [ 72],\n",
       "        [ 73]],\n",
       "\n",
       "       [[ 70],\n",
       "        [ 71],\n",
       "        [ 72],\n",
       "        [ 73],\n",
       "        [ 74]],\n",
       "\n",
       "       [[ 71],\n",
       "        [ 72],\n",
       "        [ 73],\n",
       "        [ 74],\n",
       "        [ 75]],\n",
       "\n",
       "       [[ 72],\n",
       "        [ 73],\n",
       "        [ 74],\n",
       "        [ 75],\n",
       "        [ 76]],\n",
       "\n",
       "       [[ 73],\n",
       "        [ 74],\n",
       "        [ 75],\n",
       "        [ 76],\n",
       "        [ 77]],\n",
       "\n",
       "       [[ 74],\n",
       "        [ 75],\n",
       "        [ 76],\n",
       "        [ 77],\n",
       "        [ 78]],\n",
       "\n",
       "       [[ 75],\n",
       "        [ 76],\n",
       "        [ 77],\n",
       "        [ 78],\n",
       "        [ 79]],\n",
       "\n",
       "       [[ 76],\n",
       "        [ 77],\n",
       "        [ 78],\n",
       "        [ 79],\n",
       "        [ 80]],\n",
       "\n",
       "       [[ 77],\n",
       "        [ 78],\n",
       "        [ 79],\n",
       "        [ 80],\n",
       "        [ 81]],\n",
       "\n",
       "       [[ 78],\n",
       "        [ 79],\n",
       "        [ 80],\n",
       "        [ 81],\n",
       "        [ 82]],\n",
       "\n",
       "       [[ 79],\n",
       "        [ 80],\n",
       "        [ 81],\n",
       "        [ 82],\n",
       "        [ 83]],\n",
       "\n",
       "       [[ 80],\n",
       "        [ 81],\n",
       "        [ 82],\n",
       "        [ 83],\n",
       "        [ 84]],\n",
       "\n",
       "       [[ 81],\n",
       "        [ 82],\n",
       "        [ 83],\n",
       "        [ 84],\n",
       "        [ 85]],\n",
       "\n",
       "       [[ 82],\n",
       "        [ 83],\n",
       "        [ 84],\n",
       "        [ 85],\n",
       "        [ 86]],\n",
       "\n",
       "       [[ 83],\n",
       "        [ 84],\n",
       "        [ 85],\n",
       "        [ 86],\n",
       "        [ 87]],\n",
       "\n",
       "       [[ 84],\n",
       "        [ 85],\n",
       "        [ 86],\n",
       "        [ 87],\n",
       "        [ 88]],\n",
       "\n",
       "       [[ 85],\n",
       "        [ 86],\n",
       "        [ 87],\n",
       "        [ 88],\n",
       "        [ 89]],\n",
       "\n",
       "       [[ 86],\n",
       "        [ 87],\n",
       "        [ 88],\n",
       "        [ 89],\n",
       "        [ 90]],\n",
       "\n",
       "       [[ 87],\n",
       "        [ 88],\n",
       "        [ 89],\n",
       "        [ 90],\n",
       "        [ 91]],\n",
       "\n",
       "       [[ 88],\n",
       "        [ 89],\n",
       "        [ 90],\n",
       "        [ 91],\n",
       "        [ 92]],\n",
       "\n",
       "       [[ 89],\n",
       "        [ 90],\n",
       "        [ 91],\n",
       "        [ 92],\n",
       "        [ 93]],\n",
       "\n",
       "       [[ 90],\n",
       "        [ 91],\n",
       "        [ 92],\n",
       "        [ 93],\n",
       "        [ 94]],\n",
       "\n",
       "       [[ 91],\n",
       "        [ 92],\n",
       "        [ 93],\n",
       "        [ 94],\n",
       "        [ 95]],\n",
       "\n",
       "       [[ 92],\n",
       "        [ 93],\n",
       "        [ 94],\n",
       "        [ 95],\n",
       "        [ 96]],\n",
       "\n",
       "       [[ 93],\n",
       "        [ 94],\n",
       "        [ 95],\n",
       "        [ 96],\n",
       "        [ 97]],\n",
       "\n",
       "       [[ 94],\n",
       "        [ 95],\n",
       "        [ 96],\n",
       "        [ 97],\n",
       "        [ 98]],\n",
       "\n",
       "       [[ 95],\n",
       "        [ 96],\n",
       "        [ 97],\n",
       "        [ 98],\n",
       "        [ 99]],\n",
       "\n",
       "       [[ 96],\n",
       "        [ 97],\n",
       "        [ 98],\n",
       "        [ 99],\n",
       "        [100]],\n",
       "\n",
       "       [[ 97],\n",
       "        [ 98],\n",
       "        [ 99],\n",
       "        [100],\n",
       "        [101]],\n",
       "\n",
       "       [[ 98],\n",
       "        [ 99],\n",
       "        [100],\n",
       "        [101],\n",
       "        [102]],\n",
       "\n",
       "       [[ 99],\n",
       "        [100],\n",
       "        [101],\n",
       "        [102],\n",
       "        [103]]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "xtrain,xtest,ytrain,ytest=train_test_split(data,target,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM((1),batch_input_shape=(None,5,1),return_sequences=False))#None any number of rows number of column =5, 1 is vector\n",
    "model.compile(loss='mean_absolute_error',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 0s 195us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 195us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 0us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 283us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 172us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 330us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 327us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 276us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 336us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 445us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 189us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 217us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 262us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 270us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 100us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 207us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 278us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 151us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 189us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 193us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 200us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 100us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 100us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 141us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 100us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 0s 162us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "80/80 [==============================] - 0s 187us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "80/80 [==============================] - 0s 125us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "80/80 [==============================] - 0s 162us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "80/80 [==============================] - 0s 212us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "80/80 [==============================] - 0s 125us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "80/80 [==============================] - 0s 125us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "80/80 [==============================] - 0s 125us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "80/80 [==============================] - 0s 125us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "80/80 [==============================] - 0s 125us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "80/80 [==============================] - 0s 125us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "80/80 [==============================] - 0s 125us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "80/80 [==============================] - 0s 125us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "80/80 [==============================] - 0s 125us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "80/80 [==============================] - 0s 162us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "80/80 [==============================] - 0s 137us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "80/80 [==============================] - 0s 150us/step - loss: 10.5625 - acc: 0.0000e+00 - val_loss: 12.2500 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "training=model.fit(xtrain,ytrain,epochs=100,validation_data=(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results=model.predict(xtest)\n",
    "plt.scatter(range(20),results)\n",
    "plt.scatter(range(20),ytest)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-78ba89558ade>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "plt.plot(training.training['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
